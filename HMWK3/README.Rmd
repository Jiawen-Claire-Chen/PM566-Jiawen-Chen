---
title: "HW3"
author: "Jiawen"
date: "2022-11-01"
output: 
       github_document: 
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(repos=c(CRAN="http://cran.rstudio.com"))
```

```{r}
library(rvest)
library(httr)
library(tidyverse)
library(stringr)
library(dplyr)
library(forcats)
library(ggplot2)
library(tidytext)
```
#1.1 How many papers were you able to find?
```{r}
# Downloading the website
website <- xml2::read_html("https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2+trial+vaccine")
# Finding the counts
counts <- xml2::xml_find_first(website,"/html/body/main/div[9]/div[2]/div[2]/div[1]/div[1]/span")
# Turning it into text
counts <- as.character(counts)
# Extracting the data using regex
stringr::str_extract(counts, "[0-9,]+")
```

#1.2 Using the list of pubmed ids you retrieved, download each papers’ details using the query parameter rettype = abstract. If you get more than 250 ids, just keep the first 250.
```{r}
query_ids <- GET(
  url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
  query = list(
    db= "pubmed",
    term= "sars-cov-2 trial vaccine",
    retmax= 250
  ), 
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)
```
#1.3 Create a dataset containing the following:
Pubmed ID number,
Title of the paper,
Name of the journal where it was published,
Publication date, and Abstract of the paper (if any).
```{r}
# Turn the result into a character vector
ids <- as.character(ids)
# Find all the ids 
ids <- stringr::str_extract_all(ids, "<Id>[[:digit:]]+</Id>")[[1]]
# Remove all the leading and trailing <Id> </Id>. Make use of "|"
ids <- stringr::str_remove_all(ids, "</?Id>")

head(ids)
```
Grab publications with pubmed id 
```{r}
publications <- GET(
  url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi",
  query = list(
    db= "pubmed",
    id= paste(ids,collapse = ","),
    retmax= 250,
    rettype= "abstract"
    )
)
# Turning the output into character vector
publications <- httr::content(publications)
publications_txt <- as.character(publications)
```

```{r}
pub_char_list <- xml2::xml_children(publications)
pub_char_list <- sapply(pub_char_list, as.character)
```

```{r}
abstracts <- str_extract(pub_char_list, "<Abstract>[[:print:][:space:]]+</Abstract>")
abstracts[[1]]
abstracts <- str_remove_all(abstracts, "</?[[:alnum:]- =\"]+>") 
abstracts[[1]]
abstracts <- str_replace_all(abstracts, "[[:space:]]+"," ")
abstracts[[1]]
```

title of paper
```{r}
titles <- str_extract(pub_char_list, "<ArticleTitle>[[:print:][:space:]]+</ArticleTitle>")
titles[[1]]
titles <- str_remove_all(titles, "</?[[:alnum:]- =\"]+>")
titles[[1]]
```
Name of the journal where it was published,
```{r}
Journal <- str_extract(pub_char_list, "<Title>[[:print:][:space:]]+</Title>")
Journal[[1]]
Journal <- str_remove_all(Journal, "</?[[:alnum:]- =\"]+>")
Journal[[1]]
```

```{r}
PubDate <- str_extract(pub_char_list, "<PubDate>[[:print:][:space:]]+</PubDate>")
PubDate[[1]]
PubDate <- str_remove_all(PubDate, "</?[[:alnum:]- =\"]+>")
PubDate[[1]]
PubDate <- str_replace_all(PubDate, "[[:space:]]+"," ")
PubDate[[1]]
```
Finally the dataset:
```{r}
database <- data.frame(
  PubMedId = ids,
  Title    = titles,
  Journal = Journal,
  PubDate = PubDate,
  Abstract = abstracts
)
knitr::kable(database[1:8,], caption = "Some papers about Covid19 trial vaccine")
```

#2. Read in the data
```{r}
if (!file.exists("pubmed.csv")) {
  download.file("https://raw.githubusercontent.com/USCbiostats/data-science-data/master/03_pubmed/pubmed.csv", 
                "pubmed.csv", method="libcurl", timeout = 60)
}

pubmed <- read.csv("pubmed.csv")
```

#2.1 Tokenize the abstracts and count the number of each token. Do you see anything interesting? Does removing stop words change what tokens appear as the most frequent? What are the 5 most common tokens for each search term after removing stopwords?
```{r}
pubmed %>%
  unnest_tokens(word, abstract) %>%
  count(word, sort=TRUE) %>%
  anti_join(stop_words, by = c("word")) %>%
  top_n(5,n) 
```
Stop words appear as the most frequent. After removing stop words, most frequent tokens change. COVID, 19, patients, cancer and prostate appear at the most frequent 

#2.2 Tokenize the abstracts into bigrams. Find the 10 most common bigram and visualize them with ggplot2.
```{r}
pubmed %>%
  unnest_tokens(word, abstract) %>%
  count(word, sort=TRUE) %>%
  anti_join(stop_words, by = c("word")) %>%
  top_n(10,n) %>%
  ggplot(aes(n, fct_reorder(word,n))) +
  geom_col()
```

#2.3 Calculate the TF-IDF value for each word-search term combination. (here you want the search term to be the “document”) What are the 5 tokens from each search term with the highest TF-IDF value? How are the results different from the answers you got in question 1?
```{r}
pubmed %>%
  unnest_tokens(text, abstract) %>%
  count(text, term) %>%
  bind_tf_idf(text, term, n) %>%
  arrange(desc(tf_idf)) %>%
  top_n(5,n)
```